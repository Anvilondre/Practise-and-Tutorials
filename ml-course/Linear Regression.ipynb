{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook you will implement [linear regression](https://towardsdatascience.com/how-does-linear-regression-actually-work-3297021970dd)!\n",
    "1. Fill the function to compute MSE cost using the formula below\n",
    "2. Implement `predict` function\n",
    "3. Implement gradient descend based on cost and predict functions\n",
    "4. Implement alternative solver - normal equation, using the prediction function and formula below\n",
    "5. Put it all together in `fit` method\n",
    "6. Test your model and compare it to the one from sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to predict\n",
    "$pred = X W$, where:  \n",
    "$X$ - input features vector  \n",
    "$W$ - weights  \n",
    "$pred$ - predicted value\n",
    "\n",
    "### MSE cost formula\n",
    "$cost = \\frac{1}{n}\\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^{2}$, where:  \n",
    "$n$ - number of examples  \n",
    "$Y_i$ - correct value of example $i$  \n",
    "$\\hat{Y}_i$ - predicted value of example $i$\n",
    "\n",
    "### [Gradient descent](https://builtin.com/data-science/gradient-descent)\n",
    "$grad = \\frac{2}{n}\\sum_{i=1}^{n} ((\\hat{Y}_i - Y_i) X_i)$  \n",
    "$W = W - \\alpha * grad$, where:  \n",
    "$grad$ - calculated gradient  \n",
    "$W$ - weights  \n",
    "$n$ - number of training examples  \n",
    "$Y_i$ - correct value of example $i$  \n",
    "$\\hat{Y}_i$ - predicted value of example $i$  \n",
    "$X_i$ - feature vector of example $i$\n",
    "\n",
    "### [Normal equation formula](https://medium.com/swlh/understanding-mathematics-behind-normal-equation-in-linear-regression-aa20dc5a0961)\n",
    "$W = (X^T X)^{-1} (X^T Y)$, where:  \n",
    "$W$ - weights  \n",
    "$X$ - input features  \n",
    "$Y$ - correct values\n",
    "\n",
    "### Useful [pytorch](https://pytorch.org/get-started) functions\n",
    "`torch.mm(X, Y)` - matrix multiplication $X Y$  \n",
    "`torch.sum(X)` - sum of all elements over $X$  \n",
    "`torch.abs(x)` - absolute value of x, i. e. $|x|$  \n",
    "`torch.det(X)` - determinant of $X$  \n",
    "`torch.inverse(X)` - inverse of $X$, i. e. $X^{-1}$  \n",
    "`torch.zeros(size)` - tensor filled with 0-s of shape `size`  \n",
    "`torch.ones(size)` - tensor filled with 1-s of shape `size`  \n",
    "`torch.rand(size)` - tensor of shape `size` filled with random numbers  \n",
    "`torch.cat(tensors)` - concatinates the sequence of `tensors`  \n",
    "`X.t()` - transpose of $X$ matrix  \n",
    "`X.size()` - shape of $X$ tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed packages\n",
    "import torch\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myLinearRegression:\n",
    "    def __init__(self,\n",
    "            solver='gr_d',  # Optimization method. Gradient descent by default, otherwise normal equation\n",
    "            normal_init=True,  # If True - weights will be initialized with random numbers, otherwise with zeros\n",
    "            fit_intercept=True,  # If True - bias will be added\n",
    "            learning_rate=1e-3,  # Step of the gradient descent\n",
    "            tolerance=1e-3,  # If cost change is less than tolerance - we're done with learning\n",
    "            n_steps=10000):  # Maximum number of gradient descent steps\n",
    "        self.solver = solver\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_steps = n_steps\n",
    "        self.normal_init = normal_init\n",
    "        self.tolerance = tolerance\n",
    "        self.W = None  # Initialize weights as None\n",
    "    \n",
    "    def cost(self, X, Y):\n",
    "        \"\"\"\n",
    "        Computes MSE cost. General formula is provided in the cell above\n",
    "        X - predicted values\n",
    "        Y - correct values\n",
    "        \"\"\"\n",
    "        ############### Calculate the MSE score\n",
    "        \n",
    "        \n",
    "        cost = ...\n",
    "        ###############  ~ 2-4 lines\n",
    "        return cost\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Computes the prediction on X\n",
    "        \"\"\"\n",
    "        # X must be a Tensor\n",
    "        if type(X) != torch.Tensor:\n",
    "            X = torch.from_numpy(X).float()\n",
    "            \n",
    "        if self.fit_intercept:\n",
    "            ############### Transform X into a matrix where first collumn is full of ones, and the second collumn is X\n",
    "            X = ...\n",
    "            ###############  ~ 1 line\n",
    "            \n",
    "        ############### Calculate the prediction\n",
    "        prediction = ...\n",
    "        ###############  ~ 1 line\n",
    "        return prediction\n",
    "    \n",
    "    def intercept(self):\n",
    "        \"\"\"\n",
    "        Return model's intecept (bias)\n",
    "        \"\"\"\n",
    "        if self.fit_intercept:\n",
    "            return ...  # return the intercept\n",
    "        else:\n",
    "            return None\n",
    "  \n",
    "    def gradient_descent(self, X, Y):\n",
    "        \"\"\"\n",
    "        Gradient descent solver\n",
    "        X - predicted values\n",
    "        Y - correct values\n",
    "        \"\"\"\n",
    "        ############### Set m be equal to the number of training examples\n",
    "        m = ...\n",
    "        ###############  ~ 1 line\n",
    "\n",
    "        for i in range(self.n_steps):\n",
    "            ############### Predict values, calculate cost, find the gradient and then use it to update weights\n",
    "            \n",
    "            \n",
    "            self.weights -= ...\n",
    "            \n",
    "            change = ...  # Absolute difference between old cost and new one\n",
    "            if change < self.tolerance:  # \n",
    "                break\n",
    "            ###############  ~ 6-9 lines\n",
    "\n",
    "    def normal_equation(self, X, Y):\n",
    "        \"\"\"\n",
    "        Normal equation solver\n",
    "        Computes weights using the normal equation formula (cell above)\n",
    "        Keep in mind that the determinant shouldn't be zero\n",
    "        \"\"\"\n",
    "        ############### Calculate the weigts\n",
    "        \n",
    "        \n",
    "        self.W = ...\n",
    "        ###############  ~ 1-5 lines\n",
    "\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Main method\n",
    "        Initialize parameters here\n",
    "        Then use one of the solvers you've implemented before\n",
    "        \"\"\"\n",
    "        # Convert X, Y to Tensors\n",
    "        X = torch.from_numpy(X).float()\n",
    "        Y = torch.from_numpy(Y).float().view(-1, 1)\n",
    "        \n",
    "        if self.normal_init:\n",
    "            ############### Initialize weights with random values\n",
    "            self.W = ...\n",
    "            ###############  ~ 1 line\n",
    "        else:\n",
    "            ############### Initialize weights with zeros\n",
    "            self.W = ...\n",
    "            ###############  ~ 1 line\n",
    "\n",
    "        if self.solver == 'gr_d':\n",
    "            ############### Call gradient descent solver\n",
    "            pass\n",
    "            ###############  ~ 1 line\n",
    "        else:\n",
    "            ############### Call normal equation solver\n",
    "            pass\n",
    "            ###############  ~ 1 line\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's test your model\n",
    "If everything done right:  \n",
    "- You won't get any errors  \n",
    "- MSE score of your model will be similar to the one from sklearn (about 1653)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = make_regression(n_samples=10000, n_features=5,\n",
    "                           n_targets=1, bias=2.5, noise=40, random_state=42)\n",
    "print(f'X shape: {X.shape}')\n",
    "print(f'y shape: {Y.shape}')\n",
    "\n",
    "grad_model = myLinearRegression(solver='gr_d')\n",
    "grad_model.fit(X, Y)\n",
    "grad_preds = grad_model.predict(X)\n",
    "print(f'Grad descent model MSE: {mean_squared_error(Y, grad_preds)}')\n",
    "print(f'Intercept: {grad_model.intercept}')\n",
    "\n",
    "\n",
    "# sklearn model\n",
    "sklearn_model = LinearRegression()\n",
    "sklearn_model.fit(X, Y)\n",
    "sklearn_preds = sklearn_model.predict(X)\n",
    "print(f'Sklearn MSE: {mean_squared_error(Y, sklearn_preds)}')\n",
    "print(f'Intercept: {sklearn_model.intercept_} \\nCoefs: {sklearn_model.coef_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
